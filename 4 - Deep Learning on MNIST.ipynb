{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c5bce6",
   "metadata": {},
   "source": [
    "# Deep Learning on MNIST\n",
    "\n",
    "[Resource](https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html)\n",
    "\n",
    "This tutorial demonstrates how to build a simple **feedforward neural network** (with one hidden layer) and train it from scratch with NumPy to recognize handwritten digit images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a529b",
   "metadata": {},
   "source": [
    "# Before We Begin\n",
    "\n",
    "Feedforward refers to recognition-inference architecture of neural networks. As you already know, artificial neural networks architectures are based on inputs multiplied by weights to obtain outputs.\n",
    "\n",
    "On the other hand, recurrent neural networks, or neural networks with loops, allow information from later processing stages to feed back to earlier stages for sequence processing. However, at every stage of inference a feedforward multiplication remains the core, essential for backpropagation.\n",
    "\n",
    "Thus, neural networks cannot contain feedback like negative feedback or positive feedback where the outputs feed back to the very same inputs and modify them, as this will form an infinite loop.\n",
    "\n",
    "Essentially, it's not possible to rewind in time to generate an error signal through backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5640e",
   "metadata": {},
   "source": [
    "# Continuing with the lesson\n",
    "\n",
    "Your deep learning model - one of the most basic artificial neural networks that resembles the original multi-layer perceptron - will learn to classify digits from 0 to 9 from the MNIST dataset.\n",
    "\n",
    "The dataset contains 60,000 training and 10,000 test images and corresponding labels. Each training and test image is of size 784 (or 28x28 pixels) - this will be your input for the neural network.\n",
    "\n",
    "Based on the image inputs and their labels (supervised learning), your neural network will be trained to learn their features using forward propagation and backpropagation (reverse-mode differentiation). The final output of the network is a vector of 10 scores - one for each handwritten digit image. You will also be evaluate how good your model is at classifying the images on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab872e1b",
   "metadata": {},
   "source": [
    "# Load the MNIST Dataset\n",
    "\n",
    "In this section, you will download the zipped MNIST dataset files. Then, you will transform them into 4 files of NumPy array type using built-in Python modules. Finally, you will split the arrays into training and test sets.\n",
    "\n",
    "1) Define a variable to store the training/test image/label names of the MNIST dataset in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b351f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = {\n",
    "    \"training_images\": \"train-images-idx3-ubyte.gz\",  # 60,000 training images.\n",
    "    \"test_images\": \"t10k-images-idx3-ubyte.gz\",  # 10,000 test images.\n",
    "    \"training_labels\": \"train-labels-idx1-ubyte.gz\",  # 60,000 training labels.\n",
    "    \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",  # 10,000 test labels.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b801989c",
   "metadata": {},
   "source": [
    "2) Load the data. First check if the data is stored locally; if not, then download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a05a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
    "\n",
    "for fname in data_sources.values():\n",
    "    fpath = os.path.join(data_dir, fname)\n",
    "    if not os.path.exists(fpath):\n",
    "        print(\"Downloading file: \" + fname)\n",
    "        resp = requests.get(base_url + fname, stream=True)\n",
    "        resp.raise_for_status()\n",
    "        with open(fpath, \"wb\") as fh:\n",
    "            for chunk in resp.iter_content(chunk_size=128):\n",
    "                fh.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44bf6a",
   "metadata": {},
   "source": [
    "3) Decompress the 4 files and create 4 `ndarrays`, saving them into a dictionary. Each original image is of size 28x28 and neural networks normally expect a 1D vector input; therefore, you also need to reshape the images by multiplying 28 by 28 (784)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56ae416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "mnist_dataset = {}\n",
    "\n",
    "# Images\n",
    "for key in (\"training_images\", \"test_images\"):\n",
    "    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n",
    "        mnist_dataset[key] = np.frombuffer(\n",
    "            mnist_file.read(), np.uint8, offset=16\n",
    "        ).reshape(-1, 28 * 28)\n",
    "# Labels\n",
    "for key in (\"training_labels\", \"test_labels\"):\n",
    "    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n",
    "        mnist_dataset[key] = np.frombuffer(mnist_file.read(), np.uint8, offset=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcade691",
   "metadata": {},
   "source": [
    "4) Split the data into training and test sets using the standard notation of `x` for data and `y` for labels, calling the training and test set images `x_train` and `x_test`, and the labels `y_train` and `y_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5c392ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = (\n",
    "    mnist_dataset[\"training_images\"],\n",
    "    mnist_dataset[\"training_labels\"],\n",
    "    mnist_dataset[\"test_images\"],\n",
    "    mnist_dataset[\"test_labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0746d8a4",
   "metadata": {},
   "source": [
    "5) You can confirm that the shape of the image array is `(60000, 784)` and `(10000, 784)` for training and test sets, respectively, and the labels - `(60000,)` and `(10000, )`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46436f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training images: (60000, 784) and training labels: (60000,)\n",
      "The shape of test images: (10000, 784) and test labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"The shape of training images: {x_train.shape} and training labels: {y_train.shape}\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"The shape of test images: {x_test.shape} and test labels: {y_test.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b2fc8b",
   "metadata": {},
   "source": [
    "6) And you can inspect some images using Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e002ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Take the 60,000th image (indexed at 59,999) from the training set,\n",
    "# reshape from (784, ) to (28, 28) to have a valid shape for displaying purposes.\n",
    "mnist_image = x_train[59999, :].reshape(28, 28)\n",
    "# Set the color mapping to grayscale to have a black background.\n",
    "plt.imshow(mnist_image, cmap=\"gray\")\n",
    "# Display the image.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04b923e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display 5 random images from the training set\n",
    "num_examples = 5\n",
    "seed = 147197952744\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "fig, axes = plt.subplots(1, num_examples)\n",
    "for sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes):\n",
    "    ax.imshow(sample.reshape(28, 28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff2e23",
   "metadata": {},
   "source": [
    "Above are five 3images from the MNIST training set. Various hand-drawn Arabic numerals are shows, with exact values chosen randomly with each run of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b9417",
   "metadata": {},
   "source": [
    "**Note:** You can also visualize a sample image as an array by printing `x_train[59999]`. Here, `59999` is your 60,000th training image sample. Your output will be quite long and should contain an array of 8-bit integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b817dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  38,  48,  48,  22,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,  62,  97, 198, 243, 254, 254, 212,  27,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  67, 172, 254, 254, 225, 218, 218, 237, 248,  40,   0,\n",
       "        21, 164, 187,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  89, 219, 254,  97,  67,  14,   0,   0,  92, 231,\n",
       "       122,  23, 203, 236,  59,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,  25, 217, 242,  92,   4,   0,   0,   0,   0,\n",
       "         4, 147, 253, 240, 232,  92,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0, 101, 255,  92,   0,   0,   0,   0,\n",
       "         0,   0, 105, 254, 254, 177,  11,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0, 167, 244,  41,   0,   0,\n",
       "         0,   7,  76, 199, 238, 239,  94,  10,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 192, 121,   0,\n",
       "         0,   2,  63, 180, 254, 233, 126,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 190,\n",
       "       196,  14,   2,  97, 254, 252, 146,  52,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0, 130, 225,  71, 180, 232, 181,  60,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 130, 254, 254, 230,  46,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   6,  77, 244, 254, 162,   4,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 110, 254, 218, 254, 116,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0, 131, 254, 154,  28, 213,  86,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  66, 209, 153,  19,  19, 233,\n",
       "        60,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0, 142, 254, 165,   0,\n",
       "        14, 216, 167,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  90, 254,\n",
       "       175,   0,  18, 229,  92,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        26, 229, 249, 176, 222, 244,  44,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  73, 193, 197, 134,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[59999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "441b823d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint8(8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the label of the 60,000th image from the training set\n",
    "y_train[59999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9daf49c",
   "metadata": {},
   "source": [
    "# Preprocess the Data\n",
    "\n",
    "Neural networks can work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type. When preprocessing the data, you should consider the following processes: vectorization and conversion to a floating-point format.\n",
    "\n",
    "**Vectorization** is simple converting a matrix into one vector.\n",
    "\n",
    "Since the MNIST data is already vectorized and the arrays are of `dtype` `uint8`, your next challenge is to convert them to a floating-point format, such as `float64` (double-precision):\n",
    "\n",
    "- Normalizing the image data: a feature scaling procedure that can speed up the neural network training process by standardizing the distribution of your input data.\n",
    "- One-hot/categorical encoding of the image labels.\n",
    "\n",
    "In practice, you can use different types of floating-point precision depending on your goals and you can find more information about that in the [Nvidia](https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/) and [Google Cloud](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus) blog posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a5c2f2",
   "metadata": {},
   "source": [
    "## Convert the image data to the floating-point format\n",
    "\n",
    "The images data contain 8-bit integers encoded in the [0, 255] interval with color values between 0 and 255.\n",
    "\n",
    "You will normalize them into floating-point arrays in the [0, 1] interval by dividing them by 255.\n",
    "\n",
    "1) Check that the vectorized image data has type `uint8`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a1bd712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of training images: uint8\n",
      "The data type of test images: uint8\n"
     ]
    }
   ],
   "source": [
    "print(\"The data type of training images: {}\".format(x_train.dtype))\n",
    "print(\"The data type of test images: {}\".format(x_test.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c7486",
   "metadata": {},
   "source": [
    "2) Normalize the arrays by dividing them by 255 (and thus promoting the data type from `uint8` to `float64`) and then assign the train and test image data variables - `x_train` and `x_test` - to `training_images` and `train_labels`, respectively. To reduce the model training and evaluation time in this example, only a subset of the training and test images will be used. Both `training_images` and `test_images` will contain only 1,000 samples each out of the complete datasets of 60,000 and 10,000 images, respectively. These values can be controlled by changing the `training_sample` and `test_sample` below, up to their maximum values of 60,000 and 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d80123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "training_sample, test_sample = 1000, 1000\n",
    "training_images = x_train[0:training_sample] / 255\n",
    "test_images = x_test[0:test_sample] / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fa48e",
   "metadata": {},
   "source": [
    "You can also do it this way for a more elegant solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d55417a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample, test_sample = 1000, 1000\n",
    "training_images = x_train[0:training_sample] / x_train.max()\n",
    "test_images = x_test[0:test_sample] / x_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b4b93",
   "metadata": {},
   "source": [
    "3) Confirm that the image data has changed to the floating-point format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e657fffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of training images: float64\n",
      "The data type of test images: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"The data type of training images: {}\".format(training_images.dtype))\n",
    "print(\"The data type of test images: {}\".format(test_images.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb01c9",
   "metadata": {},
   "source": [
    "**Note:** You can also check that normalization was successful by printing `training_images[0]`. Your output should contain an array of floating-point numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1270d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(training_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090bdcf",
   "metadata": {},
   "source": [
    "## Convert the labels to floating point through categorical/one-hot encoding\n",
    "\n",
    "You will use one-hot encoding to embed each digit label as an all-zero vector with `np.zeros()` and place `1` for a label index. As a result, your label data will be arrays with `1.0` (or `1.`) in the position of each image label.\n",
    "\n",
    "Since there are 10 labels (from 0 to 9) in total, your arrays will look similar to this:\n",
    "\n",
    "`array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644ae00",
   "metadata": {},
   "source": [
    "1) Confirm that the image label data are integers with `dtype` `uint8`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6671ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of training labels: uint8\n",
      "The data type of test labels: uint8\n"
     ]
    }
   ],
   "source": [
    "print(\"The data type of training labels: {}\".format(y_train.dtype))\n",
    "print(\"The data type of test labels: {}\".format(y_test.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd236750",
   "metadata": {},
   "source": [
    "2) Define a function that performs one-hot encoding on arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2f13023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels, dimension=10):\n",
    "    \"\"\"Define a one-hot variable for an all-zero vector\n",
    "    with 10 dimensions (number labels from 0 to 9)\"\"\"\n",
    "    one_hot_labels = labels[..., None] == np.arange(dimension)[None]\n",
    "    # Return one-hot encoded labels\n",
    "    return one_hot_labels.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5c509",
   "metadata": {},
   "source": [
    "3) Encode the labels and assign the values to new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d08d991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = one_hot_encoding(y_train[:training_sample])\n",
    "test_labels = one_hot_encoding(y_test[:test_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c988cb",
   "metadata": {},
   "source": [
    "4) Check that the data type has changed to floating point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98310400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of training labels: float64\n",
      "The data type of test labels: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"The data type of training labels: {}\".format(training_labels.dtype))\n",
    "print(\"The data type of test labels: {}\".format(test_labels.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20aed5",
   "metadata": {},
   "source": [
    "5) Examine a few encoded labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "600a24e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(training_labels[0])\n",
    "print(training_labels[1])\n",
    "print(training_labels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed8cb8",
   "metadata": {},
   "source": [
    "... and compare to the originals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9f4ba81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5\n",
    "0\n",
    "4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb7ef6",
   "metadata": {},
   "source": [
    "Dataset preparation is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef65e73",
   "metadata": {},
   "source": [
    "# Build and Train a Small Neural Network from Scratch\n",
    "\n",
    "In this section you will familiarize yourself with some hig-level concepts of the basic building blocks of a deep learning mode.\n",
    "\n",
    "Afterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to identify handwritten digits from the MNIST dataset with a certain level of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319793ad",
   "metadata": {},
   "source": [
    "## Neural network building blocks with NumPy\n",
    "\n",
    "- *Layers:* These building blocks work as data filters - they process data and learn representations from inputs to better predict the target outputs.\n",
    "\n",
    "You will use 1 hidden layer in your model to pass the inputs forward (forward propagation) and propagate the gradients/error derivatives of a loss function backward (backpropagation). These are input, hidden, and output layers.\n",
    "\n",
    "In the hidden (middle) and output (last) layers, the neural network model will compute the weighted sum of inputs. To compute this process, you will use NumPy's matrix multiplication function `np.dot(layer, weights)`.\n",
    "\n",
    "**Note:** For simplicity, the bias term is omitted in this example (there is no `np.dot(layer, weights) + bias`).\n",
    "\n",
    "= *Weights:* These are important adjustable parameters that the neural network fine-tunes by forward and backward propagating the data. They are optimized through a process called gradient descent. Before the model training starts, the weights are randomly initialized with NumPy's `Generator.random()`.\n",
    "\n",
    "The optimal weights should produce the highest prediction accuracy and the lowest error on the training and test sets.\n",
    "\n",
    "- *Activation function:* Deep learning models are capable of determining non-linear relationships between inputs and outputs and these non-linear functions are usually applied to the output of each layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
